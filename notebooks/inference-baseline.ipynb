{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vesuvius Challenge Inference baseline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"This notebook is an example on how to use pretrained models and submit the results","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import json\nimport logging\nimport os\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom io import StringIO\nfrom pathlib import Path\nfrom typing import Union, get_type_hints\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom dotenv import load_dotenv\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:29:02.945744Z","iopub.execute_input":"2023-04-20T05:29:02.946200Z","iopub.status.idle":"2023-04-20T05:29:05.862474Z","shell.execute_reply.started":"2023-04-20T05:29:02.946156Z","shell.execute_reply":"2023-04-20T05:29:05.861198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial structures","metadata":{}},{"cell_type":"code","source":"%%writefile .env\n\nLOG_LEVEL = INFO\nENVIRONMENT = kaggle\nTILE_SIZE = 16\n\nBATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:29:05.864949Z","iopub.execute_input":"2023-04-20T05:29:05.866101Z","iopub.status.idle":"2023-04-20T05:29:05.877123Z","shell.execute_reply.started":"2023-04-20T05:29:05.866044Z","shell.execute_reply":"2023-04-20T05:29:05.875674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_dotenv()\n\n\nTRAINING_KEYS = [\"BATCH_SIZE\", \"EPOCHS\", \"LEARNING_RATE\", \"PATIENCE\", \"CV_FOLDS\", \"FOLD_IDX\", \"TILE_SIZE\"]\n\ndataset_path_map = {\n    \"local\": \"dataset\",\n    \"kaggle\": \"/kaggle/input/vesuvius-challenge-ink-detection\",\n}\n\n\nclass AppConfigError(Exception):\n    \"\"\"Raised when there is an error in the configuration\"\"\"\n\n\nclass AppConfig:\n    \"\"\"\n    General configuration class for the project\n    Maps environment variables to class attributes\n    \"\"\"\n\n    SEED: int = 777\n    LOG_LEVEL: str\n    ENVIRONMENT: str\n    TILE_SIZE: int\n    BATCH_SIZE: int\n    EPOCHS: int = 200\n    MODEL: str = \"\"\n    LEARNING_RATE: float = 1e-3\n    CHECKPOINTS_DIR: Path = '.'\n    PATIENCE: int = 10\n    CV_FOLDS: int = 5\n    FOLD_IDX: int = -1\n    WANDB_API_KEY: str = \"\"\n\n    def __init__(self, env):\n        for field in self.__annotations__:  # pylint: disable=no-member\n            # Raise AppConfigError if required field not supplied\n            default_value = getattr(self, field, None)\n            if default_value is None and env.get(field) is None:\n                raise AppConfigError(f\"The {field} field is required\")\n\n            # Cast env var value to expected type and raise AppConfigError on failure\n            try:\n                var_type = get_type_hints(AppConfig)[field]\n                if var_type == bool:\n                    value = _parse_bool(env.get(field, default_value))\n                else:\n                    value = var_type(env.get(field, default_value))\n\n                self.__setattr__(field, value)\n            except ValueError as err:\n                raise AppConfigError(\n                    f'Unable to cast value of \"{env[field]}\" to type \"{var_type}\" for \"{field}\" field'\n                ) from err\n\n    @property\n    def NUM_WORKERS(self) -> int:\n        \"\"\"Defines the number of workers for the DataLoader\"\"\"\n        return os.cpu_count() or 0\n\n    @property\n    def DEVICE(self) -> torch.device:\n        \"\"\"Defines the device to use for training\"\"\"\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    @property\n    def DATASET_PATH(self) -> Path:\n        \"\"\"Defines the path to the dataset logic\"\"\"\n        return Path(dataset_path_map[self.ENVIRONMENT])\n\n    @property\n    def WANDB_PROJECT(self) -> str:\n        return \"Vesuvius Challenge\"\n\n    def __repr__(self):\n        attrs = {\n            **vars(self),\n            **{\n                prop_name: str(getattr(self, prop_name))\n                for prop_name in dir(self)\n                if isinstance(getattr(type(self), prop_name, None), property)\n            },\n        }\n\n        # Remove private attributes\n        attrs.pop(\"WANDB_API_KEY\", None)\n\n        attrs_str = json.dumps(attrs, indent=4, sort_keys=True, cls=ConfigEncoder)\n        return f\"{type(self).__name__}({attrs_str})\"\n\n    def __getitem__(self, key):\n        return self.__getattribute__(key)\n\nConfig = AppConfig(os.environ)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-20T05:29:05.878740Z","iopub.execute_input":"2023-04-20T05:29:05.879591Z","iopub.status.idle":"2023-04-20T05:29:05.907581Z","shell.execute_reply.started":"2023-04-20T05:29:05.879534Z","shell.execute_reply":"2023-04-20T05:29:05.906321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\nlogger.setLevel(Config.LOG_LEVEL)\nstream_handler = logging.StreamHandler()\nstream_handler.setStream(tqdm)\nlogger.addHandler(stream_handler)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:29:05.910635Z","iopub.execute_input":"2023-04-20T05:29:05.911518Z","iopub.status.idle":"2023-04-20T05:29:05.931222Z","shell.execute_reply.started":"2023-04-20T05:29:05.911465Z","shell.execute_reply":"2023-04-20T05:29:05.928715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataSet","metadata":{}},{"cell_type":"code","source":"class VesuviusOriginalDataSetTest(Dataset):\n    \"\"\"This dataset uses original data from Vesuvius Challenge.\n    No optimizations, processing, and separations applied.\n    \"\"\"\n\n    def __init__(self, fragment_path):\n        self.voxels_data = None\n        self.masked_idxs = None\n        self._load_data(fragment_path)\n\n    def _load_data(self, fragment_path):\n        logger.info(\"Loading the data...\")\n        \n        self.voxels_data, masked_idxs = self._load_fragment(fragment_path)\n        self.masked_idxs = np.array(masked_idxs, dtype=np.int16)\n\n    def _load_fragment(self, fragment_path: Path):\n        slice_paths = sorted(list((fragment_path / \"surface_volume\").glob(\"*.tif\")))\n        mask = cv2.imread(str(fragment_path / \"mask.png\"), cv2.IMREAD_GRAYSCALE).astype(bool)\n        masked_idxs = self._get_masked_idxs(mask)\n        voxels_data = np.empty((len(masked_idxs), len(slice_paths), Config.TILE_SIZE, Config.TILE_SIZE), dtype=np.uint8)\n        for i, slice_path in enumerate(tqdm(slice_paths, leave=False)):\n            # In this case, we use cv2 to load image, because it's faster than PIL\n            slice_img = cv2.imread(str(slice_path), cv2.IMREAD_UNCHANGED)\n\n            # Convert to uint8 to save memory usage\n            slice_data = (slice_img // 255).astype(np.uint8)\n\n            voxels_data[:, i, :, :] = self._split_slice(slice_data, masked_idxs)\n\n        return voxels_data, masked_idxs\n\n    def _get_masked_idxs(self, mask):\n        \"\"\"\n        Returns list of tuples with indexes of tiles with data.\n        Basically, the idea of this function is to pre calculate the indexes of tiles with data,\n        so it would be possible to pre-allocate memory for the fragments and then just fill it with data.\n        This approach is much faster and less memory consuming than just appending to the list and concatenating then.\n        \"\"\"\n        mask_idxs = []\n        for i in range(0, mask.shape[0], Config.TILE_SIZE):\n            for j in range(0, mask.shape[1], Config.TILE_SIZE):\n                if mask[i : i + Config.TILE_SIZE, j : j + Config.TILE_SIZE].any():\n                    mask_idxs.append((i, j))\n        return mask_idxs\n\n    def _split_slice(self, slice_data, masked_idxs):\n        \"\"\"Split slice into tiles. It's possible to mask to filter out tiles with no data.\"\"\"\n        tiles = np.empty((len(masked_idxs), Config.TILE_SIZE, Config.TILE_SIZE), dtype=np.uint8)\n        for k, (i, j) in enumerate(masked_idxs):\n            tile = slice_data[i : i + Config.TILE_SIZE, j : j + Config.TILE_SIZE]\n            if tile.shape != (Config.TILE_SIZE, Config.TILE_SIZE):\n                tile = np.pad(\n                    tile,\n                    (\n                        (0, Config.TILE_SIZE - tile.shape[0]),\n                        (0, Config.TILE_SIZE - tile.shape[1]),\n                    ),\n                    \"constant\",\n                    constant_values=0,\n                )\n\n            tiles[k] = tile\n\n        return tiles\n\n    def __len__(self) -> int:\n        return self.voxels_data.shape[0]\n\n    def __getitem__(self, index):\n        voxel = (self.voxels_data[index] / 255.0).astype(np.float32)\n        tile_coords = self.masked_idxs[index]\n        return torch.from_numpy(voxel).unsqueeze(0), tile_coords","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-20T05:29:05.935536Z","iopub.execute_input":"2023-04-20T05:29:05.936202Z","iopub.status.idle":"2023-04-20T05:29:05.956471Z","shell.execute_reply.started":"2023-04-20T05:29:05.936162Z","shell.execute_reply":"2023-04-20T05:29:05.955221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"class InkDetector(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        filters = [16, 32, 64]\n        paddings = [1, 1, 1]\n        kernel_sizes = [3, 3, 3]\n        strides = [2, 2, 2]\n\n        layers = []\n        in_channels = 1\n        for num_filters, padding, kernel_size, stride in zip(\n            filters, paddings, kernel_sizes, strides\n        ):\n            layers.extend(\n                [\n                    nn.Conv3d(\n                        in_channels=in_channels,\n                        out_channels=num_filters,\n                        kernel_size=kernel_size,\n                        stride=stride,\n                        padding=padding,\n                    ),\n                    nn.ReLU(inplace=True),\n                    nn.BatchNorm3d(num_features=num_filters),\n                ]\n            )\n            in_channels = num_filters\n        layers.append(nn.AdaptiveAvgPool3d(1))\n        layers.append(nn.Flatten())\n\n        self.encoder = nn.Sequential(*layers)\n        self.decoder = nn.Sequential(\n            nn.Linear(in_channels, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x):\n        features = self.encoder(x)\n        return self.decoder(features)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:29:05.957927Z","iopub.execute_input":"2023-04-20T05:29:05.958332Z","iopub.status.idle":"2023-04-20T05:29:05.980360Z","shell.execute_reply.started":"2023-04-20T05:29:05.958298Z","shell.execute_reply":"2023-04-20T05:29:05.978719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models loading","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass PretrainedModel:\n    name: str\n    model_class: nn.Module\n    folds_path: Path\n        \ninput_base = Path('../input')\npretrained_models = [\n    PretrainedModel(\"InkDetector-pure\", InkDetector, input_base / 'inknetv1')\n]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:29:05.982137Z","iopub.execute_input":"2023-04-20T05:29:05.982885Z","iopub.status.idle":"2023-04-20T05:29:06.002510Z","shell.execute_reply.started":"2023-04-20T05:29:05.982824Z","shell.execute_reply":"2023-04-20T05:29:06.001422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"result = {}\nfor fragment_path in (Config.DATASET_PATH / 'test').iterdir():\n    image_size = cv2.imread(str(fragment_path / 'mask.png')).shape[:2]\n    \n    dataset = VesuviusOriginalDataSetTest(fragment_path)\n    dataloader = DataLoader(\n        dataset, \n        batch_size=Config.BATCH_SIZE,\n        num_workers=Config.NUM_WORKERS\n    )\n    \n    ink_masks = []\n    for pretrained_model in pretrained_models:\n        model = pretrained_model.model_class().to(Config.DEVICE)\n        for fold_path in pretrained_model.folds_path.iterdir():\n            ink_mask = np.zeros(image_size, dtype=bool)\n            \n            model.load_state_dict(torch.load(fold_path, map_location=Config.DEVICE))\n            model.eval()\n    \n            for x, tile_coords in tqdm(dataloader):\n                batch_pred = model(x).detach().cpu().numpy()[:, 0] > 0.5\n                \n                for coords, pred in zip(tile_coords, batch_pred):\n                    ink_mask[coords[0]:coords[0] + Config.TILE_SIZE, coords[1]: coords[1] + Config.TILE_SIZE] = pred\n            \n            ink_masks.append(ink_mask)\n    result_mask = np.array(ink_masks).mean(axis=0) > 0.2  # Majority voting strategy\n    result[fragment_path.stem] = result_mask\n    \n    plt.imshow(result_mask)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:56:37.992695Z","iopub.execute_input":"2023-04-20T05:56:37.993665Z","iopub.status.idle":"2023-04-20T05:59:55.752896Z","shell.execute_reply.started":"2023-04-20T05:56:37.993603Z","shell.execute_reply":"2023-04-20T05:59:55.751053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(result_mask)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:52:24.585283Z","iopub.execute_input":"2023-04-20T05:52:24.585680Z","iopub.status.idle":"2023-04-20T05:52:27.709809Z","shell.execute_reply.started":"2023-04-20T05:52:24.585631Z","shell.execute_reply":"2023-04-20T05:52:27.708442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"def combined_rle(img, img_id):\n    pixels = img.flatten()\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] = runs[1::2] - runs[:-1:2]\n    f = StringIO()\n    np.savetxt(f, runs.reshape(1, -1), delimiter=\" \", fmt=\"%d\")\n    predicted = f.getvalue().strip()\n    return {\"Id\": img_id, \"Predicted\": predicted}","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:34:50.368942Z","iopub.status.idle":"2023-04-20T05:34:50.369369Z","shell.execute_reply.started":"2023-04-20T05:34:50.369169Z","shell.execute_reply":"2023-04-20T05:34:50.369191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = []\nfor name, pred_mask in result.items():\n    submission.append(combined_rle(pred_mask, name))\n\nsubmission_df = pd.DataFrame(submission)\nsubmission_df    ","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:34:50.370900Z","iopub.status.idle":"2023-04-20T05:34:50.371463Z","shell.execute_reply.started":"2023-04-20T05:34:50.371185Z","shell.execute_reply":"2023-04-20T05:34:50.371217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T05:34:50.374569Z","iopub.status.idle":"2023-04-20T05:34:50.375066Z","shell.execute_reply.started":"2023-04-20T05:34:50.374799Z","shell.execute_reply":"2023-04-20T05:34:50.374822Z"},"trusted":true},"execution_count":null,"outputs":[]}]}